# Chapter 4: SentencePiece ğŸŒ

> **"SentencePiece treats raw text as a sequence of Unicode characters â€” no language-specific rules needed."**  
> â€” Kudo & Richardson, 2018

---

## ğŸ§’ Story for a 5-Year-Old

Imagine you're trying to learn to read Chinese, Japanese, Tamil, and English all at once. In English, words have spaces between them. But in Chinese and Japanese, there are NO spaces! How do you split things up?

SentencePiece says: "I won't make any assumptions about spaces. I'll just look at the raw text as a stream of characters and figure out the chunks myself." It's like learning to read without anyone telling you where words start and end.

---

## ğŸ’¡ Why SentencePiece Exists

Traditional tokenizers assume:
- Text is pre-tokenized (split by whitespace)
- Words exist as meaningful units
- Latin scripts with clear word boundaries

These assumptions **break** for:
- **Japanese**: ã‚¹ã‚·ãŒé£Ÿã¹ãŸã„ (no spaces between words)
- **Chinese**: æˆ‘å–œæ¬¢åƒå¯¿å¸ (no spaces)
- **Thai**: à¸‰à¸±à¸™à¸Šà¸­à¸šà¸à¸´à¸™à¸‹à¸¹à¸Šà¸´ (no spaces)
- **Tamil**: à®¨à®¾à®©à¯ à®šà¯à®·à®¿ à®šà®¾à®ªà¯à®ªà®¿à®Ÿ à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯‡à®©à¯ (spaces, but complex morphology)

**SentencePiece** solves this by:
1. Treating whitespace as a regular character (normalizing it)
2. Working directly on raw Unicode characters
3. Not assuming any pre-tokenization
4. Running BPE or Unigram LM on the resulting character stream

---

## ğŸ”§ Key Design Decision: The â– (LOWER ONE EIGHTH BLOCK) Character

SentencePiece replaces whitespace with `â–` (U+2581) and treats it as part of the vocabulary.

```
Input:  "Hello World how are you"
Stored: "â–Helloâ–Worldâ–howâ–areâ–you"

Tokens might be: ["â–Hello", "â–World", "â–how", "â–are", "â–you"]
            or: ["â–Hello", "â–Wor", "ld", "â–how", "â–are", "â–you"]
```

**Why this matters:**
- "World" in the middle of text vs "â–World" at start of a word â†’ different tokens
- Allows perfect reconstruction: tokens â†’ original text (lossless)
- No need for separate pre-tokenization step

```
ASCII Diagram:

Traditional pipeline:          SentencePiece pipeline:
                               
  Raw text                       Raw text
     â”‚                              â”‚
     â–¼                              â”‚
  Whitespace split                  â”‚ (raw, no splitting)
     â”‚                              â–¼
     â–¼                         Replace space with â–
  Word list                         â”‚
     â”‚                              â–¼
     â–¼                         BPE or Unigram LM
  Subword tokenizer                 â”‚
     â”‚                              â–¼
     â–¼                         Tokens (â– marks word boundaries)
  Tokens
```

---

## ğŸŒ Language Examples

### Japanese
```
Input:     æ—¥æœ¬èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
Raw chars: æ—¥ æœ¬ èª ã® ãƒˆ ãƒ¼ ã‚¯ ãƒ³ åŒ–

Without SentencePiece: ??? (no spaces to split on!)
With SentencePiece:    ["â–æ—¥æœ¬èª", "ã®", "â–ãƒˆãƒ¼ã‚¯", "ãƒ³åŒ–"]
                    or ["â–æ—¥æœ¬", "èªã®ãƒˆãƒ¼ã‚¯", "ãƒ³åŒ–"]  (different, still valid)
```

### Chinese
```
Input:     æˆ‘å–œæ¬¢åƒå¯¿å¸
Raw chars: æˆ‘ å–œ æ¬¢ åƒ å¯¿ å¸

SentencePiece tokens: ["â–æˆ‘", "å–œæ¬¢", "åƒ", "å¯¿å¸"]
```

### Tamil
```
Input:     à®¨à®¾à®©à¯ à®ªà®Ÿà®¿à®•à¯à®•à®¿à®±à¯‡à®©à¯
SentencePiece: ["â–à®¨à®¾à®©à¯", "â–à®ªà®Ÿà®¿", "à®•à¯à®•à®¿", "à®±à¯‡à®©à¯"]
```

### English
```
Input:    "Hello World"
â†’ Stored: "â–Helloâ–World"
â†’ Tokens: ["â–Hello", "â–World"]

# Note: â–Hello â‰  Hello (different tokens!)
# "Hello" in middle of word: "saHello" â†’ ["sa", "Hello"]
```

---

## ğŸ”¢ Mathematical Treatment

SentencePiece normalizes training text:

```
normalize(text) = unicode_normalize(replace(' ', 'â–'))
```

Then trains BPE or Unigram LM directly on:
```
normalized_corpus = "â–Helloâ–Worldâ–howâ–areâ–you..."
```

The model sees `â–` as a normal character. Tokens that start with `â–` are "word-initial" by convention.

---

## âš™ï¸ System-Level Insight

### Why Google Used SentencePiece for T5, ALBERT, mT5

1. **Pipeline simplicity**: No language-specific tokenizer needed. One tool for all 101 languages in mT5.
2. **Reproducibility**: No dependency on language-specific tools (MeCab for Japanese, Jieba for Chinese, etc.)
3. **Raw byte-level fallback**: Unknown characters â†’ individual bytes â†’ never OOV
4. **Subword regularization**: Built-in Unigram LM sampling

### SentencePiece Configuration

```python
import sentencepiece as spm

# Train SentencePiece model
spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='mymodel',
    vocab_size=32000,
    model_type='bpe',          # or 'unigram'
    character_coverage=0.9995, # coverage of Unicode characters
    pad_id=0,
    unk_id=1,
    bos_id=2,
    eos_id=3,
    pad_piece='[PAD]',
    unk_piece='[UNK]',
    bos_piece='[BOS]',
    eos_piece='[EOS]',
)

# Load and use
sp = spm.SentencePieceProcessor()
sp.load('mymodel.model')

text = "Hello World how are you"
tokens = sp.encode(text, out_type=str)
print(tokens)  # ['â–Hello', 'â–World', 'â–how', 'â–are', 'â–you']

ids = sp.encode(text, out_type=int)
print(ids)     # [123, 456, 789, 234, 567]

# Decode back
decoded = sp.decode(ids)
print(decoded)  # "Hello World how are you"

# Subword regularization (sampling)
samples = [sp.encode(text, out_type=str, 
                     enable_sampling=True, alpha=0.1, nbest_size=-1)
           for _ in range(5)]
```

---

## ğŸ“Š character_coverage Parameter

Critical for multilingual models:

```
character_coverage = 0.9995  # covers 99.95% of all Unicode characters in corpus

For Latin-script languages: 0.9995 is fine
For Indic languages: use 0.9999 or higher (many unique characters)
For emoji/rare chars: lower is okay (they get byte-fallback)
```

Characters NOT covered â†’ decomposed into individual bytes (byte fallback).

---

## âš ï¸ Common Mistakes

1. **Mixing normalized and unnormalized text**: Training on NFKC but testing on NFC â†’ inconsistent tokens
2. **Forgetting â– in decoding**: Stripping tokens without replacing â– â†’ lost spacing
3. **Wrong `character_coverage` for language**: Too low â†’ OOV; too high â†’ sparse vocabulary
4. **Using `model_type='bpe'` for multilingual**: Unigram is generally better for multilingual
5. **Treating â–token â‰  token**: `"â–Hello" != "Hello"` â€” different token IDs!

---

## ğŸ”­ Research Insight

SentencePiece's design enables **zero-shot cross-lingual transfer**: if a model is trained on multiple languages with one SentencePiece model, it can generalize to unseen languages at inference time â€” because the byte-level fallback ensures all text can be represented.

---

---

# Chapter 5: Byte-Level Tokenization & Byte Fallback ğŸ”¢

> **"Every piece of text â€” in any language, with any emoji â€” can be represented as a sequence of bytes. Bytes never fail."**  
> â€” GPT-2 (Radford et al., 2019)

---

## ğŸ§’ Story for a 5-Year-Old

You know how computers secretly store everything as numbers? Even the letter "A" is actually the number 65 inside the computer. And the ğŸ• emoji is several numbers: 240, 159, 141, 149.

Byte-level tokenization says: let's just use those raw numbers (0-255) as our "alphabet"! That way, ANYTHING you type â€” any emoji, any language, any weird symbol â€” can be expressed as these 256 building blocks. Nothing is ever "unknown."

---

## ğŸ’¡ Intuition

### Why Byte-Level?

UTF-8 encodes all Unicode text as sequences of bytes (0-255). Every possible string can be expressed as a byte sequence. Therefore:

- **Vocabulary size = 256** (just the bytes)
- **No OOV ever** â€” anything can be expressed
- **Language agnostic** â€” bytes don't care about scripts

The downside: bytes are smaller than characters â†’ longer sequences â†’ more compute.

**Byte-level BPE** (GPT-2 style): Start with 256 byte tokens, then run BPE to merge frequent byte sequences into longer tokens.

---

## ğŸŒ UTF-8 Encoding

UTF-8 is a **variable-length encoding** for Unicode:

```
Character   Unicode    UTF-8 bytes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A           U+0041     41
Ã©           U+00E9     C3 A9
â‚¬           U+20AC     E2 82 AC
ä¸­          U+4E2D     E4 B8 AD
ğŸ˜€          U+1F600    F0 9F 98 80

ASCII chars (0-127):  1 byte
Latin extended:       2 bytes
Most Asian scripts:   3 bytes
Emoji / rare:         4 bytes
```

```
ASCII Diagram: UTF-8 byte structure

1 byte:  0xxxxxxx                          (U+0000 to U+007F)
2 bytes: 110xxxxx 10xxxxxx                 (U+0080 to U+07FF)
3 bytes: 1110xxxx 10xxxxxx 10xxxxxx        (U+0800 to U+FFFF)
4 bytes: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx (U+10000 to U+10FFFF)
```

---

## ğŸ”¢ Walk-Through Examples

### Example 1: ASCII
```
"hello" â†’ UTF-8 bytes â†’ [104, 101, 108, 108, 111]
         â†’ Byte tokens â†’ ['h', 'e', 'l', 'l', 'o']
         â†’ After BPE merges:
           h+e â†’ he
           he+l â†’ hel  
           hel+l â†’ hell
           hell+o â†’ hello
         â†’ [hello]  (if common enough)
```

### Example 2: Emoji ğŸ•
```
ğŸ• â†’ UTF-8 bytes â†’ [0xF0, 0x9F, 0x8D, 0x95]
               â†’ [240, 159, 141, 149]
               â†’ byte tokens: ['Ä ', 'Å', 'Å‰', 'Å•']  (GPT-2 byte encoding)
               â†’ might merge to: ['ğŸ•']  (if frequent in training data)
               â†’ or stay as 4 byte tokens (if rare)
```

### Example 3: Mixed Language
```
"Hello ä¸–ç•Œ ğŸŒ"

English:  H e l l o    â†’ 5 bytes â†’ (merges to ~2 tokens)
Space:    ' '           â†’ 1 byte
Chinese:  ä¸–(3B) ç•Œ(3B) â†’ 6 bytes â†’ (may merge to 1-2 tokens if common)
Space:    ' '           â†’ 1 byte  
Emoji:    ğŸŒ(4B)        â†’ 4 bytes â†’ (if rare, stays 4 tokens)

Total: ~14 bytes â†’ ~8-10 tokens after BPE
```

### Example 4: Rare Character
```
"ê©»" (Cham script, Myanmar)
â†’ UTF-8: [0xEA, 0xA9, 0xBB] = [234, 169, 187]
â†’ 3 byte tokens (almost certainly won't be merged â€” too rare)
â†’ Result: 3 tokens for 1 character!
```

---

## ğŸ¨ ASCII Diagram: Byte Fallback

```
Input character: "å–œ"

Standard tokenizer:
  "å–œ" in vocab? â†’ YES â†’ token ID 24853
                â†’ NO  â†’ [UNK] token!  â† BAD

Byte-level tokenizer:
  "å–œ" â†’ UTF-8 bytes: [0xE5, 0x96, 0x9C]
       â†’ byte tokens: [229, 150, 156]
       â†’ 3 tokens (always works, never UNK)
```

---

## ğŸ“Š Token Efficiency: Language Comparison

GPT-4 tokenizer (cl100k_base):

```
Script          Chars/token    Example
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
English         ~4 chars       "programming" = 1 token
German          ~3.5 chars     "programmierung" = 2 tokens
Chinese (CJK)   ~1.5 chars    "ç¨‹åºè®¾è®¡" = ~3 tokens
Arabic          ~1.2 chars    "Ø¨Ø±Ù…Ø¬Ø©" = ~4 tokens
Tamil           ~0.8 chars    "à®¨à®¿à®°à®²à®¾à®•à¯à®•à®®à¯" = ~10 tokens
Emoji           ~0.25 chars   "ğŸ‰" = 1 token (if in vocab)
               
Relative cost (English = 1.0x):
  English:  1.0x
  German:   1.1x
  Chinese:  2.5x
  Arabic:   3.5x
  Tamil:    5-10x
```

---

## âš–ï¸ Trade-offs: Robustness vs Token Efficiency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BYTE-LEVEL BPE                       â”‚
â”‚                                                        â”‚
â”‚  PROS:                         CONS:                   â”‚
â”‚  âœ“ Zero OOV ever               âœ— Longer sequences      â”‚
â”‚  âœ“ Handles all languages        âœ— More compute cost    â”‚
â”‚  âœ“ Handles all emoji            âœ— Less semantic per    â”‚
â”‚  âœ“ Robust to typos             â”‚  token initially      â”‚
â”‚  âœ“ Simple (256 base tokens)    âœ— Byte boundaries may   â”‚
â”‚  âœ“ No Unicode normalization    â”‚  cut through chars    â”‚
â”‚    required                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Impact on Sequence Length

For the SAME content:

```
Standard BPE (32k vocab):  ~N tokens
Byte-level BPE (32k vocab): ~1.1N tokens for English
                            ~2-3N tokens for CJK scripts
                            ~4-10N tokens for Tamil/Arabic (unfamiliar chars)
```

This directly impacts:
- Attention FLOPs: O(nÂ²) â†’ longer = much more expensive
- Context window utilization: fewer "ideas" fit in same context
- Generation speed: more steps = slower

---

## ğŸ” GPT-2 Byte-Level BPE Details

GPT-2 uses a specific byte encoding where all 256 bytes are mapped to printable Unicode characters:

```python
def bytes_to_unicode():
    """
    GPT-2's mapping of bytes to Unicode characters.
    Ensures all 256 bytes have a unique printable representation.
    """
    bs = (list(range(ord("!"), ord("~")+1)) +
          list(range(ord("Â¡"), ord("Â¬")+1)) +
          list(range(ord("Â®"), ord("Ã¿")+1)))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))

# Space (byte 32) â†’ 'Ä '
# 'A' (byte 65) â†’ 'A'
# Tab (byte 9) â†’ 'Ä‰'
```

Then BPE merges are run on this "bytified" text.

---

## ğŸ’» Byte Tokenization Demo

```python
def text_to_bytes(text: str) -> list:
    """Convert text to UTF-8 bytes."""
    return list(text.encode('utf-8'))

def bytes_to_text(byte_list: list) -> str:
    """Convert bytes back to text."""
    return bytes(byte_list).decode('utf-8', errors='replace')

def analyze_languages():
    """Compare byte lengths across languages."""
    examples = {
        "English": "Hello World programming",
        "Chinese": "ä½ å¥½ä¸–ç•Œç¼–ç¨‹",
        "Arabic":  "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…",
        "Tamil":   "à®µà®£à®•à¯à®•à®®à¯ à®‰à®²à®•à®®à¯",
        "Emoji":   "ğŸŒğŸ‰ğŸš€ğŸ’»",
        "Japanese": "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",
        "Mixed":   "Hello ä¸–ç•Œ ğŸŒ",
    }
    
    print(f"{'Language':<12} {'Text':<25} {'Chars':>6} {'Bytes':>6} {'B/C':>6}")
    print("-" * 60)
    
    for lang, text in examples.items():
        chars = len(text)
        byte_len = len(text.encode('utf-8'))
        ratio = byte_len / chars
        print(f"{lang:<12} {text:<25} {chars:>6} {byte_len:>6} {ratio:>6.2f}")

analyze_languages()

# Show UTF-8 byte breakdown
def show_utf8(char):
    """Show UTF-8 bytes for a character."""
    bts = char.encode('utf-8')
    hex_repr = ' '.join(f'{b:02X}' for b in bts)
    dec_repr = ' '.join(str(b) for b in bts)
    print(f"'{char}' (U+{ord(char):04X}): bytes [{hex_repr}] = [{dec_repr}]")

print("\nUTF-8 byte breakdown:")
for char in ['A', 'Ã©', 'ä¸­', 'ğŸ˜€', 'à®©']:
    show_utf8(char)
```

---

## ğŸ”¬ Research: ByT5 â€” Pure Byte-Level Transformer

**ByT5** (Xue et al., 2022) operates directly on raw bytes â€” no tokenization step at all!

```
Architecture:
  Input bytes â†’ [byte embeddings] â†’ long encoder sequence
                                  â†’ compressed latent
                                  â†’ shorter decoder sequence

Performance: Competitive with T5 on many benchmarks
Advantage:   Zero tokenization artifacts, perfect multilingual coverage
Disadvantage: 4-8x longer sequences â†’ much more compute
```

Key finding: byte-level models are **more robust to noise, typos, and character-level attacks** than subword models.

---

## âš ï¸ Common Mistakes

1. **Confusing characters and bytes**: `len("ğŸŒ")` = 1 char but 4 bytes in Python 3
2. **Naive byte splitting**: Not respecting UTF-8 multi-byte boundaries â†’ garbled characters
3. **Ignoring the efficiency cost**: Byte-level models need longer context windows for same content
4. **Not using BPE on top of bytes**: Pure byte vocabulary (256 tokens) gives terrible efficiency; need BPE merges on top

---

## ğŸ”­ Open Research Problems

1. **Efficient byte models**: Hierarchical byte processing (encode bytes â†’ compress â†’ process)
2. **Adaptive granularity**: Different "zoom levels" per token based on information content
3. **Cross-modal byte models**: Can byte-level models naturally handle binary data, images, audio?
4. **Learned byte groupings**: Learn optimal byte merges language-specifically

---

*Next: Chapter 6 â€” Why Tamil, Arabic, and CJK Fragment More â†’*
